# VÃ©libâ€™ Daily Insights â€“ Data Pipeline AWS

## ğŸ“Œ Contexte
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du cours **Data Pipeline Cloud (EPISEN)**.  
Lâ€™objectif est de concevoir un **pipeline de donnÃ©es automatisÃ© sur AWS**, Ã  partir des donnÃ©es Open Data VÃ©libâ€™, afin de produire des indicateurs mÃ©tier exploitables via un dashboard.

---

## ğŸ—ï¸ Architecture GÃ©nÃ©rale
Pipeline simple et automatisÃ© :

EventBridge â†’ Lambda â†’ S3 (Raw) â†’ Glue Crawler â†’ Athena â†’ QuickSight

- Ingestion rÃ©guliÃ¨re des donnÃ©es VÃ©libâ€™
- Stockage des donnÃ©es brutes dans S3
- Catalogage automatique avec Glue
- RequÃªtes analytiques avec Athena
- Visualisation des KPI via QuickSight

---

## ğŸŒ RÃ©gion AWS
- **RÃ©gion utilisÃ©e** : `us-east-1` 
> âš ï¸ Toutes les ressources doivent Ãªtre crÃ©Ã©es dans cette rÃ©gion.

---

## ğŸ§© Convention de nommage

### Projet
- **Project name** : `velib-insights`

### S3
- **Bucket principal** : `velib-insights-naw-seu-2326`

---

## âš™ï¸ Ressources AWS

### Lambda
- **Nom** : `velib_ingest_lambda`
- **RÃ´le** : rÃ©cupÃ©ration des donnÃ©es VÃ©libâ€™ via lâ€™API Open Data et Ã©criture dans S3

### EventBridge
- **Rule** : `velib_ingest_schedule`
- **FrÃ©quence** : toutes les **15 minutes**

### AWS Glue
- **Database** : `velib_db`
- **Crawler** : `velib_raw_crawler`
- **RÃ´le** : catalogage automatique des donnÃ©es S3

### Athena
- **Workgroup** : `velib_workgroup`
- **RÃ´le** : requÃªtes SQL pour calcul des KPI

---

## ğŸ·ï¸ Tags AWS
Les ressources AWS sont taguÃ©es avec les clÃ©s suivantes :

- `project` = `velib-insights`
- `owner` = `team-naw-seu`
- `course` = `data-pipeline-episen`

---

## ğŸ“Š KPI analysÃ©s
- Taux de remplissage des stations
- Stations en pÃ©nurie (0 vÃ©lo disponible)
- Stations saturÃ©es (0 borne libre ou > 90%)
- Top 10 stations critiques
- Analyse par arrondissement
- Analyse par tranche horaire

---

## ğŸ‘¥ Organisation de lâ€™Ã©quipe
- **Infra & Ingestion** : S3, Lambda, EventBridge, Glue, Terraform
- **Analytics & Visualisation** : Athena (SQL), QuickSight, KPI, slides client

---

## ğŸš€ DÃ©ploiement
Lâ€™infrastructure est dÃ©ployÃ©e via **Infrastructure as Code (Terraform)**.  
Un seul environnement AWS est utilisÃ© (contrainte du lab Ã©tudiant).

---

## ğŸ“ Remarques
- Projet conÃ§u comme un **PoC client**
- Architecture volontairement simple, robuste et peu coÃ»teuse
- Aucun Machine Learning utilisÃ© (conformÃ©ment aux attentes du cours)

---

## ğŸ“ Cours
EPISEN â€“ Data Pipeline Cloud  
AnnÃ©e universitaire 2025â€“2026